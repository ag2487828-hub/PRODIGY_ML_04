# PRODIGY_ML_04

🤚 Hand Gesture Recognition Model

This project focuses on developing a Hand Gesture Recognition Model that can accurately identify and classify different hand gestures from image or video data. Such models play a crucial role in intuitive human-computer interaction, gesture-based control systems, and can be applied to areas such as sign language recognition, gaming interfaces, and virtual/augmented reality environments.


📂 Dataset

We have used the LeapGestRecog Dataset available on Kaggle:
🔗https://www.kaggle.com/gti-upm/leapgestrecog

The dataset consists of 200,000 images of 10 different gestures performed by multiple users, captured using the Leap Motion Controller.


🚀 Project Workflow

1. Data Collection & Preprocessing
Load images from dataset
Convert to grayscale
Normalize pixel values
Resize images for consistency

2. Exploratory Data Analysis (EDA)
Visualize sample gesture images
Analyze class distribution

3. Model Development
Implement Convolutional Neural Networks (CNNs) for feature extraction
Train the model to classify gestures into respective categories

4. Evaluation
Accuracy, Precision, Recall, and F1-score
Confusion Matrix for performance analysis

5. Deployment (optional)
Real-time gesture recognition using webcam or live feed


🛠️ Tech Stack

Programming Language: Python 🐍
Libraries/Frameworks:
TensorFlow / Keras
OpenCV
NumPy, Pandas
Matplotlib / Seaborn


📊 Results

Achieved high accuracy in gesture classification using CNN
Model successfully identifies and classifies 10 different hand gestures
Potential for real-time deployment in HCI applications


💡 Applications

✋ Sign Language Recognition
🎮 Gaming Control Systems
🖥️ Touchless Human-Computer Interaction
🥽 Virtual Reality (VR) and Augmented Reality (AR)
📱 Smart Home & IoT Device Control

📧 Contact

For any queries or collaborations, feel free to reach out:
👤Creator: Anshika Gupta
🔗 GitHub: ag2487828-hub
