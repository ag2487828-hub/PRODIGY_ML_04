# PRODIGY_ML_04

ğŸ¤š Hand Gesture Recognition Model

This project focuses on developing a Hand Gesture Recognition Model that can accurately identify and classify different hand gestures from image or video data. Such models play a crucial role in intuitive human-computer interaction, gesture-based control systems, and can be applied to areas such as sign language recognition, gaming interfaces, and virtual/augmented reality environments.


ğŸ“‚ Dataset

We have used the LeapGestRecog Dataset available on Kaggle:
ğŸ”—https://www.kaggle.com/gti-upm/leapgestrecog

The dataset consists of 200,000 images of 10 different gestures performed by multiple users, captured using the Leap Motion Controller.


ğŸš€ Project Workflow

1. Data Collection & Preprocessing
Load images from dataset
Convert to grayscale
Normalize pixel values
Resize images for consistency

2. Exploratory Data Analysis (EDA)
Visualize sample gesture images
Analyze class distribution

3. Model Development
Implement Convolutional Neural Networks (CNNs) for feature extraction
Train the model to classify gestures into respective categories

4. Evaluation
Accuracy, Precision, Recall, and F1-score
Confusion Matrix for performance analysis

5. Deployment (optional)
Real-time gesture recognition using webcam or live feed


ğŸ› ï¸ Tech Stack

Programming Language: Python ğŸ
Libraries/Frameworks:
TensorFlow / Keras
OpenCV
NumPy, Pandas
Matplotlib / Seaborn


ğŸ“Š Results

Achieved high accuracy in gesture classification using CNN
Model successfully identifies and classifies 10 different hand gestures
Potential for real-time deployment in HCI applications


ğŸ’¡ Applications

âœ‹ Sign Language Recognition
ğŸ® Gaming Control Systems
ğŸ–¥ï¸ Touchless Human-Computer Interaction
ğŸ¥½ Virtual Reality (VR) and Augmented Reality (AR)
ğŸ“± Smart Home & IoT Device Control

ğŸ“§ Contact

For any queries or collaborations, feel free to reach out:
ğŸ‘¤Creator: Anshika Gupta
ğŸ”— GitHub: ag2487828-hub
